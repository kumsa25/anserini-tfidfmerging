{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_queries = \"covid\"\n",
    "\n",
    "if exp_queries == \"51-100\":\n",
    "    qrel_path = \"src/main/resources/topics-and-qrels/qrels.adhoc.51-100.txt\"\n",
    "    query_path = \"src/main/resources/topics-and-qrels/topics.adhoc.51-100.txt\"\n",
    "    index_path = \"indexes/lucene-index.disk12\"\n",
    "elif exp_queries == \"covid\":\n",
    "    qrel_path = \"src/main/resources/topics-and-qrels/qrels.covid-round1.txt\"\n",
    "    query_path = \"src/main/resources/topics-and-qrels/topics.covid-round1.xml\"\n",
    "    index_path = \"indexes/lucene-index-cord19-abstract-2020-07-16/\"\n",
    "    \n",
    "searcher = LuceneSearcher(index_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_id : raw query words\n",
    "all_query_words = {}\n",
    "# query_id : [relevant document ids]\n",
    "all_qrels = {}\n",
    "\n",
    "if exp_queries == \"51-100\":\n",
    "    with open(query_path, \"r\") as f:\n",
    "        current_id = None\n",
    "        for line in f:\n",
    "            if \"Number:\" in line:\n",
    "                current_id = str(int(line.split()[2]))\n",
    "                all_query_words[current_id] = []\n",
    "            if \"Topic:\" in line:\n",
    "                query = line.split()[2:]\n",
    "                all_query_words[current_id] = \" \".join(query)\n",
    "\n",
    "elif exp_queries == \"covid\":\n",
    "    with open(query_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"<topic number=\" in line:\n",
    "                current_id = re.sub(\"[^0-9]\", \"\", line)\n",
    "                all_query_words[current_id] = []\n",
    "            if \"<query>\" in line:\n",
    "                line = re.sub(\"<query>\", \"\", line)\n",
    "                line = re.sub(\"</query>\", \"\", line)\n",
    "                line = re.sub(\"\\n\", \"\", line)\n",
    "                all_query_words[current_id] = line\n",
    "\n",
    "with open(qrel_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        query_id, _, doc_id, _ = line.split()\n",
    "        if query_id not in all_qrels:\n",
    "            all_qrels[query_id] = []\n",
    "        all_qrels[query_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform search, collect all document ids that ARE NOT returned by the query\n",
    "# query_id : [relevant document ids not returned by search]\n",
    "\n",
    "missing_docids = {}\n",
    "topk=1000\n",
    "for query_id in all_query_words:\n",
    "    missing_docids[query_id] = []\n",
    "    query = all_query_words[query_id]\n",
    "    rel_docs = set(all_qrels[query_id])\n",
    "    hits = searcher.search(query, k=topk)\n",
    "    hits_ids = set([hit.docid for hit in hits])\n",
    "    missing_docids[query_id] = list(rel_docs.difference(hits_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025\n",
      "0\n",
      "3818\n"
     ]
    }
   ],
   "source": [
    "# collect doc text for all missing docs\n",
    "# for all docs, make broad word index (map word to doc id occurrences)\n",
    "# doc_id : doc text\n",
    "\n",
    "all_needed = {}\n",
    "inverted_index = {}\n",
    "\n",
    "for x in missing_docids:\n",
    "    for docid in missing_docids[x]:\n",
    "        all_needed[docid] = None\n",
    "print(len(all_needed))\n",
    "\n",
    "for i,docid in enumerate(list(all_needed.keys())):\n",
    "    doc = searcher.doc(docid)\n",
    "    if doc == None:\n",
    "        del all_needed[docid]\n",
    "    else:\n",
    "        doc_text = doc.raw()\n",
    "        doc_text = doc_text.lower()\n",
    "        doc_text = re.sub(\"[^a-z ]\", \" \", doc_text)\n",
    "        doc_words = [x for x in doc_text.split() if len(x) > 3]\n",
    "        all_needed[docid] = doc_words\n",
    "\n",
    "        for word in doc_words:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = []\n",
    "            inverted_index[word].append(docid)\n",
    "\n",
    "    if i % 10000 == 0: print(i)\n",
    "print(len(all_needed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "query_expansion_scores = {}\n",
    "for query_id in missing_docids:\n",
    "    rel_ids = {x: None for x in missing_docids[query_id] if x in all_needed}\n",
    "    # start with words in relevant documents\n",
    "    all_rel_words = {}\n",
    "    for doc_id in rel_ids:\n",
    "        for word in all_needed[doc_id]:\n",
    "            all_rel_words[word] = {\"rel_score\": 0, \"else_score\": 0}\n",
    "\n",
    "    for word in all_rel_words:\n",
    "        rel_score = 0\n",
    "        else_score = 0\n",
    "        for occ_id in inverted_index[word]:\n",
    "            if occ_id in rel_ids:\n",
    "                all_rel_words[word][\"rel_score\"] += 1\n",
    "            else:\n",
    "                all_rel_words[word][\"else_score\"] += 1\n",
    "    query_expansion_scores[query_id] = all_rel_words\n",
    "    print(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, for each query, find the words with the highest ratio\n",
    "expanded_queries = {}\n",
    "for query_id in query_expansion_scores:\n",
    "    # collect scores for each expansion word\n",
    "    sorted_words = []\n",
    "    for word in query_expansion_scores[query_id]:\n",
    "        score = query_expansion_scores[query_id][word][\"rel_score\"] / (query_expansion_scores[query_id][word][\"else_score\"]+1)\n",
    "        if query_expansion_scores[query_id][word][\"rel_score\"] > 100:\n",
    "            sorted_words.append((word, score))\n",
    "    sorted_words = sorted(sorted_words, reverse=True, key=lambda x: x[1])[:10]\n",
    "    expanded_queries[query_id] = {\"query\": all_query_words[query_id], \"expansion_terms\": [x[0] for x in sorted_words]}\n",
    "json.dump(expanded_queries, open(\"expanded_queries/optimal.\" + exp_queries + \".json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make various expansion datasets FOR PASSING DIRECTLY TO BM25\n",
    "# weight 0, weight 1, weight 5, weight 10\n",
    "expanded_queries = json.load(open(\"expanded_queries/optimal.\" + exp_queries + \".json\", \"r\"))\n",
    "\n",
    "weights = [0,1,5,10]\n",
    "\n",
    "for weight in weights:\n",
    "    lines = []\n",
    "    with open(query_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "\n",
    "    if exp_queries == \"51-100\":\n",
    "        current_id = None\n",
    "        for i,line in enumerate(lines):\n",
    "            if \"Number:\" in line:\n",
    "                current_id = str(int(line.split()[2]))\n",
    "            if \"Topic:\" in line:\n",
    "                line = line[:-1] # remove new line\n",
    "                split_line = line.split()\n",
    "                query = split_line[2:]\n",
    "                meta_data = \" \".join(split_line[0:2])\n",
    "                line  = meta_data\n",
    "                for j in range(weight):\n",
    "                    line += \" \" + \" \".join(query)\n",
    "                query_expansion_terms = expanded_queries[current_id][\"expansion_terms\"]\n",
    "                line += \" \" + \" \".join(query_expansion_terms)\n",
    "                lines[i] = line + \"\\n\"\n",
    "        with open(\"expanded_queries/optimal.disk12.bm25.\" + str(weight) + \".51-100.txt\", \"w\") as f:\n",
    "            for line in lines:\n",
    "                f.write(line)\n",
    "\n",
    "    elif exp_queries == \"covid\":\n",
    "        for i,line in enumerate(lines):\n",
    "            if \"<topic number=\" in line:\n",
    "                current_id = re.sub(\"[^0-9]\", \"\", line)\n",
    "            if \"<query>\" in line:\n",
    "                query = re.sub(\"<query>\", \"\", line)\n",
    "                query = re.sub(\"</query>\\n\", \"\", query)\n",
    "                new_line = \"\"\n",
    "                for j in range(weight):\n",
    "                    new_line += query + \" \"\n",
    "                query_expansion_terms = expanded_queries[current_id][\"expansion_terms\"]\n",
    "                new_line += \" \" + \" \".join(query_expansion_terms)\n",
    "                lines[i] = \"\\t<query>\" + new_line + \"</query>\\n\"\n",
    "        with open(\"expanded_queries/optimal.covid.bm25.\" + str(weight) + \".xml\", \"w\") as f:\n",
    "            for line in lines:\n",
    "                f.write(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
