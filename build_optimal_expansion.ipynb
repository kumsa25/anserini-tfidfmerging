{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjros2/.local/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_queries = \"51-100\"\n",
    "exp_queries = \"covid\"\n",
    "\n",
    "if exp_queries == \"51-100\":\n",
    "    qrel_path = \"src/main/resources/topics-and-qrels/qrels.adhoc.51-100.txt\"\n",
    "    query_path = \"src/main/resources/topics-and-qrels/topics.adhoc.51-100.txt\"\n",
    "    index_path = \"indexes/lucene-index.disk12\"\n",
    "elif exp_queries == \"covid\":\n",
    "    qrel_path = \"src/main/resources/topics-and-qrels/qrels.covid-round1.txt\"\n",
    "    query_path = \"src/main/resources/topics-and-qrels/topics.covid-round1.xml\"\n",
    "    index_path = \"indexes/lucene-index-cord19-abstract-2020-07-16/\"\n",
    "    \n",
    "searcher = LuceneSearcher(index_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_id : raw query words\n",
    "all_query_words = {}\n",
    "# query_id : [relevant document ids]\n",
    "all_qrels = {}\n",
    "\n",
    "if exp_queries == \"51-100\":\n",
    "    with open(query_path, \"r\") as f:\n",
    "        current_id = None\n",
    "        for line in f:\n",
    "            if \"Number:\" in line:\n",
    "                current_id = str(int(line.split()[2]))\n",
    "                all_query_words[current_id] = []\n",
    "            if \"Topic:\" in line:\n",
    "                query = line.split()[2:]\n",
    "                all_query_words[current_id] = \" \".join(query)\n",
    "\n",
    "elif exp_queries == \"covid\":\n",
    "    with open(query_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"<topic number=\" in line:\n",
    "                current_id = re.sub(\"[^0-9]\", \"\", line)\n",
    "                all_query_words[current_id] = []\n",
    "            if \"<query>\" in line:\n",
    "                line = re.sub(\"<query>\", \"\", line)\n",
    "                line = re.sub(\"</query>\", \"\", line)\n",
    "                line = re.sub(\"\\n\", \"\", line)\n",
    "                all_query_words[current_id] = line\n",
    "\n",
    "with open(qrel_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        query_id, _, doc_id, _ = line.split()\n",
    "        if query_id not in all_qrels:\n",
    "            all_qrels[query_id] = []\n",
    "        all_qrels[query_id].append(doc_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is for optimal approach #2: Contextual expansion using relevance judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjros2/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function that encodes document, encodes query, and findest most similar latter tokens to former token\n",
    "\n",
    "lower_bound = 0.5\n",
    "upper_bound = 0.7\n",
    "considered_k = 50\n",
    "max_k = 5\n",
    "\n",
    "expansion_terms = {}\n",
    "for q,query_id in enumerate(all_query_words):\n",
    "    print(query_id)\n",
    "    query_text = all_query_words[query_id].lower()\n",
    "    query_text = re.sub(\"[^a-z0-9 ]\", \" \", query_text)\n",
    "    print(query_text)\n",
    "    doc_id = all_qrels[query_id][0]\n",
    "    doc = searcher.doc(doc_id)\n",
    "    if doc == None:\n",
    "        print(\"\\tMissing\")\n",
    "        continue\n",
    "    doc_text = doc.raw().lower()\n",
    "    doc_text = re.sub(\"[^a-z0-9 ]\", \" \", doc_text)\n",
    "\n",
    "    expansion_terms[query_id] = {}\n",
    "\n",
    "    split_query_text = query_text.split()\n",
    "    split_doc_text = doc_text.split()\n",
    "\n",
    "    embeddings_query = model.encode(split_query_text, convert_to_tensor=True)\n",
    "    embeddings_doc = model.encode(split_doc_text, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(embeddings_query, embeddings_doc)\n",
    "\n",
    "    top_k = torch.topk(cosine_scores, k=considered_k, dim=1)\n",
    "\n",
    "    for i,query_word in enumerate(split_query_text):\n",
    "        expansion_terms[query_id][query_word] = {}\n",
    "        top_doc_indices = top_k.indices[i]\n",
    "        top_doc_scores = top_k.values[i]\n",
    "        for j in range(considered_k):\n",
    "            word_idx = top_doc_indices[j]\n",
    "            exp_word = split_doc_text[word_idx]\n",
    "            word_score = top_doc_scores[j].item()\n",
    "            if word_score < upper_bound and word_score > lower_bound and len(exp_word) > 2:\n",
    "                expansion_terms[query_id][query_word][exp_word] = word_score\n",
    "                print(\"\\t\", query_word, exp_word)\n",
    "            if len(expansion_terms[query_id][query_word]) > max_k: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expansion_terms, open(\"expanded_queries/\" + \"optimal_v2.\" + exp_queries + \".json\", \"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is for optimal approach #1: keyword-based optimal expansion based on non-returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform search, collect all document ids that ARE NOT returned by the query\n",
    "# query_id : [relevant document ids not returned by search]\n",
    "\n",
    "# missing_docids is map from query_id to list of doc_ids\n",
    "\n",
    "missing_docids = {}\n",
    "topk=1000\n",
    "for query_id in all_query_words:\n",
    "    missing_docids[query_id] = []\n",
    "    query = all_query_words[query_id]\n",
    "    rel_docs = set(all_qrels[query_id])\n",
    "    hits = searcher.search(query, k=topk)\n",
    "    hits_ids = set([hit.docid for hit in hits])\n",
    "    missing_docids[query_id] = list(rel_docs.difference(hits_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025\n",
      "0\n",
      "3818\n"
     ]
    }
   ],
   "source": [
    "# collect doc text for all missing docs\n",
    "# for all docs, make broad word index (map word to doc id occurrences)\n",
    "# doc_id : doc text\n",
    "\n",
    "all_needed = {} # map from docid to list of doc words\n",
    "inverted_index = {} # map from word to docid\n",
    "\n",
    "for x in missing_docids:\n",
    "    for docid in missing_docids[x]:\n",
    "        all_needed[docid] = None\n",
    "print(len(all_needed))\n",
    "\n",
    "for i,docid in enumerate(list(all_needed.keys())):\n",
    "    doc = searcher.doc(docid)\n",
    "    if doc == None:\n",
    "        del all_needed[docid]\n",
    "    else:\n",
    "        doc_text = doc.raw()\n",
    "        doc_text = doc_text.lower()\n",
    "        doc_text = re.sub(\"[^a-z ]\", \" \", doc_text)\n",
    "        doc_words = list(set([x for x in doc_text.split() if len(x) > 3]))\n",
    "        all_needed[docid] = doc_words\n",
    "\n",
    "        for word in doc_words:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = []\n",
    "            inverted_index[word].append(docid)\n",
    "\n",
    "    if i % 10000 == 0: print(i)\n",
    "print(len(all_needed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For approach #3:\n",
    "- Sart with relevant, non-matching\n",
    "- Find words with high df > 30% of non-matching\n",
    "- Select representative based on query \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjros2/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {}\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stopwords = {word.strip(\"\\n\"): True for word in f.readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# missing_docids is map from query_id to list of doc_ids\n",
    "# all_needed is map from docid to list of doc words\n",
    "# inverted_index is map from word to docid\n",
    "# all_query_words is map from query_id to raw query\n",
    "\"\"\"\n",
    "for each query\n",
    "    for each doc id in the query\n",
    "        add words to dict counter\n",
    "\n",
    "    sort by most frequent\n",
    "    compute similarity between most frequent and query\n",
    "    once we hit threshold > 0.8? save to most similar term\n",
    "\n",
    "\"\"\"\n",
    "expansion_terms = {}\n",
    "for query_id in missing_docids:\n",
    "\n",
    "    query_text = all_query_words[query_id].lower()\n",
    "    query_text = re.sub(\"[^a-z0-9 ]\", \" \", query_text)\n",
    "    split_query = query_text.split()\n",
    "\n",
    "    all_words = {}\n",
    "    for doc_id in missing_docids[query_id]:\n",
    "        for word in all_needed.get(doc_id, []):\n",
    "            if word in stopwords: continue\n",
    "            if word in split_query: continue\n",
    "            all_words[word] = all_words.get(word, 0) + 1\n",
    "    sorted_words = sorted([(word, all_words[word]) for word in all_words], reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    considered_k = 50\n",
    "    top_words = [x[0] for x in sorted_words[:considered_k]]\n",
    "\n",
    "    \n",
    "\n",
    "    embeddings_query = model.encode(split_query, convert_to_tensor=True)\n",
    "    embeddings_doc = model.encode(top_words, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(embeddings_query, embeddings_doc)\n",
    "\n",
    "    top_k = torch.topk(cosine_scores, k=considered_k, dim=1)\n",
    "\n",
    "\n",
    "    expansion_terms[query_id] = {}\n",
    "\n",
    "    # 0. Fix stemming bug on bm25-s\n",
    "    # 1. scatterplot of individual query scores\n",
    "    # 2. normalize scores to sum to weight, redo experiments\n",
    "    # 3. only expand along one query term\n",
    "\n",
    "\n",
    "    for i,query_word in enumerate(split_query):\n",
    "        expansion_terms[query_id][query_word] = {}\n",
    "        top_doc_indices = top_k.indices[i]\n",
    "        top_doc_scores = top_k.values[i]\n",
    "        for j in range(considered_k):\n",
    "            word_idx = top_doc_indices[j]\n",
    "            exp_word = top_words[word_idx]\n",
    "            word_score = top_doc_scores[j].item()\n",
    "            if word_score < 0.5: continue\n",
    "            if len(expansion_terms[query_id][query_word]) > 4: continue\n",
    "            expansion_terms[query_id][query_word][exp_word] = word_score\n",
    "\n",
    "    print(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expansion_terms, open(\"expanded_queries/\" + \"optimal_v3.\" + exp_queries + \".json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_scores = {}\n",
    "for query_id in missing_docids:\n",
    "    rel_ids = {x: None for x in missing_docids[query_id] if x in all_needed}\n",
    "    # start with words in relevant documents\n",
    "    all_rel_words = {}\n",
    "    for doc_id in rel_ids:\n",
    "        for word in all_needed[doc_id]:\n",
    "            all_rel_words[word] = {\"rel_score\": 0, \"else_score\": 0}\n",
    "\n",
    "    for word in all_rel_words:\n",
    "        rel_score = 0\n",
    "        else_score = 0\n",
    "        for occ_id in inverted_index[word]:\n",
    "            if occ_id in rel_ids:\n",
    "                all_rel_words[word][\"rel_score\"] += 1\n",
    "            else:\n",
    "                all_rel_words[word][\"else_score\"] += 1\n",
    "    query_expansion_scores[query_id] = all_rel_words\n",
    "    print(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, for each query, find the words with the highest ratio\n",
    "expanded_queries = {}\n",
    "for query_id in query_expansion_scores:\n",
    "    # collect scores for each expansion word\n",
    "    sorted_words = []\n",
    "    for word in query_expansion_scores[query_id]:\n",
    "        score = query_expansion_scores[query_id][word][\"rel_score\"] / (query_expansion_scores[query_id][word][\"else_score\"]+1)\n",
    "        if query_expansion_scores[query_id][word][\"rel_score\"] > 100:\n",
    "            sorted_words.append((word, score))\n",
    "    sorted_words = sorted(sorted_words, reverse=True, key=lambda x: x[1])[:10]\n",
    "    expanded_queries[query_id] = {\"query\": all_query_words[query_id], \"expansion_terms\": [x[0] for x in sorted_words]}\n",
    "json.dump(expanded_queries, open(\"expanded_queries/optimal.\" + exp_queries + \".json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make various expansion datasets FOR PASSING DIRECTLY TO BM25\n",
    "# weight 0, weight 1, weight 5, weight 10\n",
    "expanded_queries = json.load(open(\"expanded_queries/optimal.\" + exp_queries + \".json\", \"r\"))\n",
    "\n",
    "weights = [0,1,5,10]\n",
    "\n",
    "for weight in weights:\n",
    "    lines = []\n",
    "    with open(query_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "\n",
    "    if exp_queries == \"51-100\":\n",
    "        current_id = None\n",
    "        for i,line in enumerate(lines):\n",
    "            if \"Number:\" in line:\n",
    "                current_id = str(int(line.split()[2]))\n",
    "            if \"Topic:\" in line:\n",
    "                line = line[:-1] # remove new line\n",
    "                split_line = line.split()\n",
    "                query = split_line[2:]\n",
    "                meta_data = \" \".join(split_line[0:2])\n",
    "                line  = meta_data\n",
    "                for j in range(weight):\n",
    "                    line += \" \" + \" \".join(query)\n",
    "                query_expansion_terms = expanded_queries[current_id][\"expansion_terms\"]\n",
    "                line += \" \" + \" \".join(query_expansion_terms)\n",
    "                lines[i] = line + \"\\n\"\n",
    "        with open(\"expanded_queries/optimal.disk12.bm25.\" + str(weight) + \".51-100.txt\", \"w\") as f:\n",
    "            for line in lines:\n",
    "                f.write(line)\n",
    "\n",
    "    elif exp_queries == \"covid\":\n",
    "        for i,line in enumerate(lines):\n",
    "            if \"<topic number=\" in line:\n",
    "                current_id = re.sub(\"[^0-9]\", \"\", line)\n",
    "            if \"<query>\" in line:\n",
    "                query = re.sub(\"<query>\", \"\", line)\n",
    "                query = re.sub(\"</query>\\n\", \"\", query)\n",
    "                new_line = \"\"\n",
    "                for j in range(weight):\n",
    "                    new_line += query + \" \"\n",
    "                query_expansion_terms = expanded_queries[current_id][\"expansion_terms\"]\n",
    "                new_line += \" \" + \" \".join(query_expansion_terms)\n",
    "                lines[i] = \"\\t<query>\" + new_line + \"</query>\\n\"\n",
    "        with open(\"expanded_queries/optimal.covid.bm25.\" + str(weight) + \".xml\", \"w\") as f:\n",
    "            for line in lines:\n",
    "                f.write(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
