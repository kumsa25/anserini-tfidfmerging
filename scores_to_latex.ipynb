{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk & Queries & Expansion Source & Weighting & Method & MAP & P@30 & R@1000 \\\\\n",
      "\\hline\n",
      "cran & 1-365 & wordnet & wordnet\\_wup & bm25s & 0.0114 & 0.0077 & 0.6415\\\\\n",
      "cran & 1-365 & wordnet & wordnet\\_path & bm25s & 0.0114 & 0.0077 & 0.6415\\\\\n",
      "cran & 1-365 & wordnet & contextual\\_mean & bm25s & 0.0116 & 0.0077 & 0.6364\\\\\n",
      "cran & 1-365 & wordnet & 0 & bm25s & 0.0115 & 0.0083 & 0.6351\\\\\n",
      "cran & 1-365 & wordnet & 0.1 & bm25s & 0.0113 & 0.0077 & 0.6340\\\\\n",
      "cran & 1-365 & wordnet & 0.5 & bm25s & 0.0113 & 0.0081 & 0.6320\\\\\n",
      "cran & 1-365 & wordnet & contextual\\_term & bm25s & 0.0113 & 0.0081 & 0.6261\\\\\n",
      "cran & 1-365 & wordnet & 1 & bm25s & 0.0108 & 0.0075 & 0.6254\\\\\n",
      "disk12 & 51-100 & wordnet & contextual\\_mean & bm25s & 0.2004 & 0.4220 & 0.4522\\\\\n",
      "disk12 & 51-100 & wordnet & 0.5 & bm25s & 0.1958 & 0.4207 & 0.4442\\\\\n",
      "disk12 & 51-100 & wordnet & contextual\\_term & bm25s & 0.1980 & 0.4220 & 0.4419\\\\\n",
      "disk12 & 51-100 & wordnet & wordnet\\_wup & bm25s & 0.1952 & 0.4340 & 0.4407\\\\\n",
      "disk12 & 51-100 & wordnet & wordnet\\_path & bm25s & 0.1952 & 0.4340 & 0.4407\\\\\n",
      "disk12 & 51-100 & wordnet & 0.1 & bm25s & 0.1905 & 0.4213 & 0.4375\\\\\n",
      "disk12 & 51-100 & wordnet & 0 & bm25s & 0.1905 & 0.4260 & 0.4253\\\\\n",
      "disk12 & 51-100 & wordnet & 1 & bm25s & 0.1568 & 0.3533 & 0.3975\\\\\n",
      "covid & round1 & wordnet & 0.5 & bm25s & 0.0235 & 0.0756 & 0.3036\\\\\n",
      "covid & round1 & wordnet & contextual\\_mean & bm25s & 0.0239 & 0.0744 & 0.3030\\\\\n",
      "covid & round1 & wordnet & contextual\\_term & bm25s & 0.0230 & 0.0778 & 0.3030\\\\\n",
      "covid & round1 & wordnet & 0.1 & bm25s & 0.0237 & 0.0756 & 0.3004\\\\\n",
      "covid & round1 & wordnet & 0 & bm25s & 0.0251 & 0.0733 & 0.2999\\\\\n",
      "covid & round1 & wordnet & wordnet\\_wup & bm25s & 0.0227 & 0.0778 & 0.2933\\\\\n",
      "covid & round1 & wordnet & wordnet\\_path & bm25s & 0.0227 & 0.0778 & 0.2933\\\\\n",
      "covid & round1 & wordnet & 1 & bm25s & 0.0209 & 0.0733 & 0.2819\\\\\n"
     ]
    }
   ],
   "source": [
    "#path_to_scores = \"runs_automatic_optimalv3_stemindex_nostemquery/\"\n",
    "#path_to_scores = \"runs_automatic_optimalv3_nostem/\"\n",
    "path_to_scores = \"runs_final/without_stemming/-pickAvgIDF/\"\n",
    "overleaf_output = []\n",
    "for file in os.listdir(path_to_scores):\n",
    "    if file[-6:] == \"scores\":\n",
    "\n",
    "        # bm25, optimal\n",
    "        #if not(\"bm25.\" in file and \".optimal.\" in file): \n",
    "        #    continue\n",
    "\n",
    "        # bm25, non optimal\n",
    "        #if not(\"wordnet.\" in file and \"bm25.\" in file):\n",
    "        #    continue\n",
    "\n",
    "        # bm25s, optimal\n",
    "        #if not(\"optimal.\" in file and \"bm25s.\" in file):\n",
    "        #    continue\n",
    "\n",
    "        # bm25s, not optimal\n",
    "        if not(\"wordnet.\" in file and \"bm25s.\" in file):\n",
    "           continue\n",
    "\n",
    "\n",
    "        scores = []\n",
    "        with open(path_to_scores + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                scores.append(line.strip(\"\\n\").split())\n",
    "        split_scores = file.split(\".\")\n",
    "        disk = split_scores[1]\n",
    "        queries = split_scores[2]\n",
    "        source = split_scores[3]\n",
    "        weight = split_scores[4]\n",
    "        if split_scores[5] in [\"1\",\"5\"]:\n",
    "            weight += \".\" + split_scores[5]\n",
    "            method = split_scores[6]\n",
    "        else:\n",
    "            method = split_scores[5]\n",
    "\n",
    "        if \"_\" in weight:\n",
    "            weight = \"\\_\".join(weight.split(\"_\"))\n",
    "        map =  scores[0][2]\n",
    "        p30 = scores[1][2]\n",
    "        recall = scores[2][2]\n",
    "\n",
    "        overleaf_output.append([disk,queries,source,weight,method,map,p30,recall])\n",
    "\n",
    "\n",
    "sorted_overleaf_output = sorted(overleaf_output, reverse=True, key=lambda x: float(x[-1]))\n",
    "sorted_overleaf_output = [\"Disk & Queries & Expansion Source & Weighting & Method & MAP & P@30 & R@1000 \\\\\\\\\"] + [\"\\\\hline\"] + [\" & \".join(x) + \"\\\\\\\\\" for x in sorted_overleaf_output]\n",
    "for line in sorted_overleaf_output:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
