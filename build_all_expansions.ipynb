{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjros2/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 0, 'method': 'bm25'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 0.1, 'method': 'bm25'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 0.5, 'method': 'bm25'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 1, 'method': 'bm25'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 'keep_optimal', 'method': 'bm25'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 'norm_across_query', 'method': 'bm25'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 0, 'method': 'bm25'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 0.1, 'method': 'bm25'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 0.5, 'method': 'bm25'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 1, 'method': 'bm25'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 'keep_optimal', 'method': 'bm25'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 'norm_across_query', 'method': 'bm25'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 0, 'method': 'bm25s'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 0.1, 'method': 'bm25s'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 0.5, 'method': 'bm25s'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 1, 'method': 'bm25s'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 'keep_optimal', 'method': 'bm25s'}\n",
      "{'corpus': 'disk12', 'queries': '51-100', 'source': 'optimal', 'weight': 'norm_across_query', 'method': 'bm25s'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 0, 'method': 'bm25s'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 0.1, 'method': 'bm25s'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 0.5, 'method': 'bm25s'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 1, 'method': 'bm25s'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 'keep_optimal', 'method': 'bm25s'}\n",
      "{'corpus': 'covid', 'queries': 'round1', 'source': 'optimal', 'weight': 'norm_across_query', 'method': 'bm25s'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TODO missing optimal expansion to bm25s\n",
    "\n",
    "# corpus disk12\n",
    "# corpus covid\n",
    "\n",
    "# queries 51-100\n",
    "# queries round1\n",
    "\n",
    "# source wordnet\n",
    "# source optimal\n",
    "\n",
    "# weight wordnet_wup\n",
    "# weight wordnet_path\n",
    "# weight contextual_mean\n",
    "# weight contextual_term\n",
    "# weight 0, 0.1, 0.5, 1\n",
    "# weight norm_across_query\n",
    "\n",
    "\n",
    "# method bm25 (use sanjeev's new changes, missing query terms in expansion given weight 1)\n",
    "# method bm25s (requires assignment, cannot be done by optimal yet)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "(DONE) wordnet, bm25s\n",
    "wordnet, bm25\n",
    "\n",
    "(DONE) optimal, bm25s\n",
    "optimal, bm25\n",
    "\n",
    "\n",
    "All wordnet paths should work\n",
    "optimal, 0/0.1/0.5/1, bm25\n",
    "\"\"\"\n",
    "\n",
    "#config = json.load(open(\"config_wordnet.json\"))\n",
    "config = json.load(open(\"config_optimal.json\"))\n",
    "\n",
    "# config has one of each above\n",
    "for run in config:\n",
    "    print(run)\n",
    "\n",
    "    corpus = run[\"corpus\"]\n",
    "    queries = run[\"queries\"]\n",
    "    source = run[\"source\"]\n",
    "    weight = run[\"weight\"]\n",
    "    method = run[\"method\"]\n",
    "\n",
    "    #if source != \"optimal\": continue\n",
    "    if weight != \"norm_across_query\": continue\n",
    "\n",
    "\n",
    "    # load queries for regular trec\n",
    "    if queries == \"51-100\":\n",
    "        path_to_queries = \"tools/topics-and-qrels/topics.adhoc.51-100.txt\"\n",
    "        all_query_words = {}\n",
    "        with open(path_to_queries, \"r\") as f:\n",
    "            current_id = None\n",
    "            for line in f:\n",
    "                #  '<num> Number:  051 \\n',\n",
    "                if \"Number:\" in line:\n",
    "                    current_id = str(int(line.split()[2]))\n",
    "                    all_query_words[current_id] = {}\n",
    "                if \"Topic:\" in line:\n",
    "                    query = line.split()[2:]\n",
    "                    for query_word in query:\n",
    "                        query_word = re.sub(\"[^A-Za-z]\", \"\", query_word.lower())\n",
    "                        if current_id is not None:\n",
    "                            all_query_words[current_id][query_word] = {}\n",
    "    elif queries == \"round1\":\n",
    "        # load queries for covid\n",
    "        path_to_queries = \"src/main/resources/topics-and-qrels/topics.covid-round1.xml\"\n",
    "        all_query_words = {}\n",
    "        with open(path_to_queries, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"<topic number=\" in line:\n",
    "                    current_id = str(int(re.sub(\"[^0-9]\", \"\", line)))\n",
    "                    all_query_words[current_id] = {}\n",
    "                if \"<query>\" in line:\n",
    "                    line = re.sub(\"<query>\", \"\", line)\n",
    "                    line = re.sub(\"</query>\", \"\", line)\n",
    "                    for query_word in line.split():\n",
    "                        query_word = re.sub(\"[^A-Za-z]\", \"\", query_word.lower())\n",
    "                        all_query_words[current_id][query_word] = {}\n",
    "\n",
    "\n",
    "    if source == \"wordnet\":\n",
    "        for query_id in all_query_words:\n",
    "            for query_word in all_query_words[query_id]:\n",
    "                synonyms = wn.synsets(query_word)\n",
    "                if len(synonyms) == 0: continue\n",
    "                orig_query_term_synset = synonyms[0]\n",
    "                all_synonyms = list(set([synonym.name().split(\".\")[0] for synonym in synonyms]))\n",
    "                all_synonyms = {x: 0 for x in all_synonyms if x != query_word}\n",
    "                all_query_words[query_id][query_word] = all_synonyms\n",
    "\n",
    "                \n",
    "\n",
    "    elif source == \"optimal\":\n",
    "        # now when they're loaded, they're in the correct format\n",
    "        if corpus == \"covid\":\n",
    "            all_query_words = json.load(open(\"expanded_queries/optimal_v3.covid.json\", \"r\"))\n",
    "        elif corpus == \"disk12\":\n",
    "            all_query_words = json.load(open(\"expanded_queries/optimal_v3.51-100.json\", \"r\"))\n",
    "\n",
    "\n",
    "    if weight in [\"wordnet_path\", \"wordnet_wup\"]:\n",
    "        for query_id in all_query_words:\n",
    "            for query_word in all_query_words[query_id]:\n",
    "                synonyms = wn.synsets(query_word)\n",
    "                if len(synonyms) == 0: continue\n",
    "                orig_query_term_synset = synonyms[0]\n",
    "                term_synset_map = {syn.name().split(\".\")[0]: syn for syn in synonyms if syn.name().split(\".\")[0] != query_word}\n",
    "                for expansion_term in all_query_words[query_id][query_word]:\n",
    "                    if weight == \"wordnet_path\":\n",
    "                        sim = orig_query_term_synset.path_similarity(term_synset_map[expansion_term])\n",
    "                    elif weight == \"wordnet_wup\":\n",
    "                        sim = orig_query_term_synset.path_similarity(term_synset_map[expansion_term])\n",
    "                    all_query_words[query_id][query_word][expansion_term] = sim\n",
    "\n",
    "    elif weight in [\"contextual_mean\", \"contextual_term\"]:\n",
    "        for query_id in all_query_words:\n",
    "            for query_word in all_query_words[query_id]:\n",
    "                expansion_terms = list(all_query_words[query_id][query_word].keys())\n",
    "                if expansion_terms == []: continue\n",
    "                embeddings_exp = model.encode(expansion_terms, convert_to_tensor=True)\n",
    "                if weight == \"contextual_term\":\n",
    "                    embeddings_orig = model.encode([query_word], convert_to_tensor=True)\n",
    "                elif weight == \"contextual_mean\":\n",
    "                    embeddings_orig = model.encode([\" \".join(list(all_query_words[query_id].keys()))], convert_to_tensor=True)\n",
    "                cosine_scores = util.cos_sim(embeddings_orig, embeddings_exp)\n",
    "                for i,exp_term in enumerate(expansion_terms):\n",
    "                    all_query_words[query_id][query_word][exp_term] = max(0,float(cosine_scores[0][i]))\n",
    "\n",
    "\n",
    "    elif weight in [0,0.1,0.5,1]:\n",
    "        for query_id in all_query_words:\n",
    "            for query_word in all_query_words[query_id]:\n",
    "                for expansion_term in all_query_words[query_id][query_word]:\n",
    "                    all_query_words[query_id][query_word][expansion_term] = weight\n",
    "\n",
    "    elif weight == \"norm_across_query\":\n",
    "        for query_id in all_query_words:\n",
    "            total_added_weight = 0\n",
    "            for query_word in all_query_words[query_id]:\n",
    "                for expansion_term in all_query_words[query_id][query_word]:\n",
    "                    total_added_weight += all_query_words[query_id][query_word][expansion_term]\n",
    "            \n",
    "            if total_added_weight == 0:\n",
    "                continue\n",
    "            else:\n",
    "                for query_word in all_query_words[query_id]:\n",
    "                    for expansion_term in all_query_words[query_id][query_word]:\n",
    "                        all_query_words[query_id][query_word][expansion_term] = all_query_words[query_id][query_word][expansion_term] / total_added_weight\n",
    "        \n",
    "    elif weight == \"norm_across_term\":\n",
    "        pass\n",
    "    elif weight == \"keep_optimal\":\n",
    "        pass\n",
    "\n",
    "    out_path = \"expanded_queries/\" + corpus + \".\" + queries + \".\" + source + \".\" + str(weight) + \".\" + method + \".txt\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        for query_id in all_query_words:\n",
    "            for query_word in all_query_words[query_id]:\n",
    "                if method == \"bm25\":\n",
    "                    for expansion_term in all_query_words[query_id][query_word]:\n",
    "                        f.write(query_id + \"-\" + expansion_term + \" \" + str(all_query_words[query_id][query_word][expansion_term]) + \"\\n\")\n",
    "                elif method == \"bm25s\":\n",
    "                    expansion_output = \", \".join([\"(\" + expansion_term + \", \" + str(all_query_words[query_id][query_word][expansion_term]) + \")\" for expansion_term in all_query_words[query_id][query_word]])\n",
    "                    if expansion_output == \"\": continue\n",
    "                    f.write(query_id + \"-\" + query_word + \" \" + expansion_output + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
